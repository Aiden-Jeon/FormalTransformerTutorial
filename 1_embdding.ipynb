{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Embedding\n",
    "\n",
    "- Algorithm 1\n",
    "\n",
    "![img](../assets/algorithm_1.png)\n",
    "\n",
    "In Algorithm 1, token embedding gets an input $v$, which represents a token ID.  \n",
    "And its output is $e$, the vector representation of the token mapped by $W_{e}$, the token embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing Algorithm 1 we need to make $W_{e}$, the token embedding matrix.  \n",
    "$W_{e}$ needs a two argument from its defintion:\n",
    "$$\n",
    "W_{e}\\in\\mathbb{R}^{d_{e} \\times N_{V}}\n",
    "$$\n",
    "- $d_{e}$ : dimension of embedding vector\n",
    "- $N_{V}$ : number of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token_embedding_matrix(d, v):\n",
    "    ...\n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dimension = 10\n",
    "num_vocab = 3\n",
    "embed_matrix = generate_token_embedding_matrix(embed_dimension, num_vocab)\n",
    "embed_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed token to vector\n",
    "Define a function that implements algorithm 1.  \n",
    "Function has two arguments, `token_id` and `embed_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_embedding(token_id, embed_matrix):\n",
    "    ...\n",
    "    return output_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_token_id = 1\n",
    "sample_output_vector = token_embedding(sample_token_id, embed_matrix)\n",
    "sample_output_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In sentence\n",
    "\n",
    "But in practical, we are not mapping only a token.\n",
    "We should map all tokens in sentence.\n",
    "\n",
    "Input sample will looks like below:\n",
    "\n",
    "```\n",
    "sample_sentence = [0, 1, 2, 3, ...]\n",
    "len(sample_sentence)\n",
    "# n\n",
    "```\n",
    "\n",
    "* `n`: length of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = [0, 1, 2]\n",
    "len(sample_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that implements mapping to all tokens in sentence.  \n",
    "Use `token_embedding` that we have defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_token_embedding(sentence, embed_matrix):\n",
    "    ...\n",
    "    return embed_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded sentence should be looks like below:\n",
    "\n",
    "```\n",
    "embed_sentence = mapping(sentence, embed)\n",
    "embed_sentence\n",
    "# [\n",
    "#     [0.xxx, 0.xxx, 0.xxx, ...],\n",
    "#     [0.xxx, 0.xxx, 0.xxx, ...],\n",
    "#     [0.xxx, 0.xxx, 0.xxx, ...],\n",
    "#     ...\n",
    "# ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_token_sentence = sentence_token_embedding(sample_sentence, embed_matrix)\n",
    "embed_token_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of `embed_sentence` is `n` same as length of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embed_token_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension of embeded vector is `d`, which is given parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embed_token_sentence[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Embedding\n",
    "\n",
    "- Algorithm 2\n",
    "\n",
    "![img](../assets/algorithm_2.png)\n",
    "\n",
    "In Algorithm 2, positional embedding gets an input $l$, which reprensents position of a token in the sequence.  \n",
    "And its ouput is $e_{p}$, the vector representation of the position mapped by $W_{p}$, the positional embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postional Embedding Matrix\n",
    "Before implementing Algorithm 2 we need to make $W_{p}$, the positional embedding matrix.\n",
    "$W_{p}$ needs a two argument from its defintion:\n",
    "$$\n",
    "W_{p}\\in\\mathbb{R}^{d_{e} \\times l_{max}}\n",
    "$$\n",
    "- $d_{e}$ : dimension of embedding vector\n",
    "- $l_{max}$ : maximal context length\n",
    "\n",
    "Definition of $l_{max}$:\n",
    "\n",
    "![img](../assets/chunking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardcoded Embedding Matrix\n",
    "In this tutorial `positional_embedding_matrix` implements original tranformer methods.\n",
    "\n",
    "> Not all transformers make use of learned positional embeddings, some use a hard-coded mapping.  \n",
    "> Such hardcoded positional embeddings can (theoretically) handle arbitrarily long sequences.  \n",
    "> The original [Transformer](https://arxiv.org/abs/1706.03762) uses:  \n",
    ">$$\n",
    ">W_{p}[2i-1,t] = \\sin({t/l^{2i/d_{e}}_{max}})\n",
    ">$$\n",
    ">$$\n",
    ">W_{p}[2i,t] = \\cos({t/l^{2i/d_{e}}_{max}})\n",
    ">$$\n",
    "> for $0 < i \\le d_{e}/2$.\n",
    "\n",
    "* $t$ represents the position of token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, assume that we have embedding vector with 10 dimension.  \n",
    "Comparing with paper's notation, $d_{e}=10 \\rightarrow 0 < i \\le 5$.\n",
    "\n",
    "And also assume that $l_{max}=5$.  \n",
    "Implementing positional_embedding_matrix expression above eash can calucated like this:\n",
    "\n",
    "- $i=1 \\rightarrow W_{p}[1,t]=\\sin(t/5^{2/10}), W_{p}[2,t]=\\cos(t/5^{2/10})$\n",
    "- $i=2 \\rightarrow W_{p}[3,t]=\\sin(t/5^{4/10}), W_{p}[4,t]=\\cos(t/5^{4/10})$\n",
    "- $i=3 \\rightarrow W_{p}[5,t]=\\sin(t/5^{6/10}), W_{p}[6,t]=\\cos(t/5^{6/10})$\n",
    "- $i=4 \\rightarrow W_{p}[7,t]=\\sin(t/5^{8/10}), W_{p}[8,t]=\\cos(t/5^{8/10})$\n",
    "- $i=5 \\rightarrow W_{p}[9,t]=\\sin(t/5^{10/10}), W_{p}[10,t]=\\cos(t/5^{10/10})$\n",
    "\n",
    "In this tutorial we assume that position $t$ has range $0\\le t < l_{max}$.  \n",
    "So it takes only two arguments `d` and `l_max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_positional_embedding_matrix(d, l_max):\n",
    "    ...\n",
    "    return pos_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_max = 5\n",
    "pos_matrix = generate_positional_embedding_matrix(embed_dimension, l_max)\n",
    "pos_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positonal embedding of a token is usually added to the token embedding to form a tokens's initial embeding.  \n",
    "For the $t$-th token of a sequence $x$, the embedding is:\n",
    "$$\n",
    "e=W_{e}[:,x[t]]+W_{p}[:,x[t]]\n",
    "$$\n",
    "\n",
    "Define a function that adds embedding vector and positional vector when `position` and `token_id` is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_by_position(token_id, position, embed_matrix, pos_matrix):\n",
    "    ...\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 0\n",
    "token_id = 0\n",
    "embed_vecotr = embed_by_position(token_id, position, embed_matrix, pos_matrix)\n",
    "embed_vecotr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained before, in practical, we should map all tokens in sentence.  \n",
    "Define a function that implements mapping to all tokens in sentence.  \n",
    "Use `embed_by_position` that we have defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence, embed_matrix, pos_matrix):\n",
    "    ...\n",
    "    return embed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_sentence = sentence_embedding(sample_sentence, embed_matrix, pos_matrix)\n",
    "embed_sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7366666ad7bf0c26a0665a94626a455744caa8a9b32341b6d3aaf8b0adc3aa10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
