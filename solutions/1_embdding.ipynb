{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Embedding\n",
    "\n",
    "- Algorithm 1\n",
    "\n",
    "![img](../assets/algorithm_1.png)\n",
    "\n",
    "In Algorithm 1, token embedding gets an input $v$, which represents a token ID.  \n",
    "And its output is $e$, the vector representation of the token mapped by $W_{e}$, the token embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing Algorithm 1 we need to make $W_{e}$, the token embedding matrix.  \n",
    "$W_{e}$ needs a two argument from its defintion:\n",
    "$$\n",
    "W_{e}\\in\\mathbb{R}^{d_{e} \\times N_{V}}\n",
    "$$\n",
    "- $d_{e}$ : dimension of embedding vector\n",
    "- $N_{V}$ : number of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token_embedding_matrix(d, v):\n",
    "    embed_matrix = {i: [i]*d for i in range(v)}\n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 1: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 2: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dimension = 10\n",
    "num_vocab = 3\n",
    "embed_matrix = generate_token_embedding_matrix(embed_dimension, num_vocab)\n",
    "embed_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed token to vector\n",
    "Define a function that implements algorithm 1.  \n",
    "Function has two arguments, `token_id` and `embed_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_embedding(token_id, embed_matrix):\n",
    "    output_vector = embed_matrix[token_id]\n",
    "    return output_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_token_id = 1\n",
    "sample_output_vector = token_embedding(sample_token_id, embed_matrix)\n",
    "sample_output_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In sentence\n",
    "\n",
    "But in practical, we are not mapping only a token.\n",
    "We should map all tokens in sentence.\n",
    "\n",
    "Input sample will looks like below:\n",
    "\n",
    "```\n",
    "sample_sentence = [0, 1, 2, 3, ...]\n",
    "len(sample_sentence)\n",
    "# n\n",
    "```\n",
    "\n",
    "* `n`: length of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence = [0, 1, 2]\n",
    "len(sample_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that implements mapping to all tokens in sentence.  \n",
    "Use `token_embedding` that we have defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_token_embedding(sentence, embed_matrix):\n",
    "    embed_sentence = []\n",
    "    for token_id in sentence:\n",
    "        embed_vector = token_embedding(token_id, embed_matrix)\n",
    "        embed_sentence.append(embed_vector)\n",
    "    return embed_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded sentence should be looks like below:\n",
    "\n",
    "```\n",
    "embed_sentence = mapping(sentence, embed)\n",
    "embed_sentence\n",
    "# [\n",
    "#     [0.xxx, 0.xxx, 0.xxx, ...],\n",
    "#     [0.xxx, 0.xxx, 0.xxx, ...],\n",
    "#     [0.xxx, 0.xxx, 0.xxx, ...],\n",
    "#     ...\n",
    "# ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_token_sentence = sentence_token_embedding(sample_sentence, embed_matrix)\n",
    "embed_token_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of `embed_sentence` is `n` same as length of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed_token_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension of embeded vector is `d`, which is given parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed_token_sentence[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Embedding\n",
    "\n",
    "- Algorithm 2\n",
    "\n",
    "![img](../assets/algorithm_2.png)\n",
    "\n",
    "In Algorithm 2, positional embedding gets an input $l$, which reprensents position of a token in the sequence.  \n",
    "And its ouput is $e_{p}$, the vector representation of the position mapped by $W_{p}$, the positional embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postional Embedding Matrix\n",
    "Before implementing Algorithm 2 we need to make $W_{p}$, the positional embedding matrix.\n",
    "$W_{p}$ needs a two argument from its defintion:\n",
    "$$\n",
    "W_{p}\\in\\mathbb{R}^{d_{e} \\times l_{max}}\n",
    "$$\n",
    "- $d_{e}$ : dimension of embedding vector\n",
    "- $l_{max}$ : maximal context length\n",
    "\n",
    "Definition of $l_{max}$:\n",
    "\n",
    "![img](../assets/chunking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardcoded Embedding Matrix\n",
    "In this tutorial `positional_embedding_matrix` implements original tranformer methods.\n",
    "\n",
    "> Not all transformers make use of learned positional embeddings, some use a hard-coded mapping.  \n",
    "> Such hardcoded positional embeddings can (theoretically) handle arbitrarily long sequences.  \n",
    "> The original [Transformer](https://arxiv.org/abs/1706.03762) uses:  \n",
    ">$$\n",
    ">W_{p}[2i-1,t] = \\sin({t/l^{2i/d_{e}}_{max}})\n",
    ">$$\n",
    ">$$\n",
    ">W_{p}[2i,t] = \\cos({t/l^{2i/d_{e}}_{max}})\n",
    ">$$\n",
    "> for $0 < i \\le d_{e}/2$.\n",
    "\n",
    "* $t$ represents the position of token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, assume that we have embedding vector with 10 dimension.  \n",
    "Comparing with paper's notation, $d_{e}=10 \\rightarrow 0 < i \\le 5$.\n",
    "\n",
    "And also assume that $l_{max}=5$.  \n",
    "Implementing positional_embedding_matrix expression above eash can calucated like this:\n",
    "\n",
    "- $i=1 \\rightarrow W_{p}[1,t]=\\sin(t/5^{2/10}), W_{p}[2,t]=\\cos(t/5^{2/10})$\n",
    "- $i=2 \\rightarrow W_{p}[3,t]=\\sin(t/5^{4/10}), W_{p}[4,t]=\\cos(t/5^{4/10})$\n",
    "- $i=3 \\rightarrow W_{p}[5,t]=\\sin(t/5^{6/10}), W_{p}[6,t]=\\cos(t/5^{6/10})$\n",
    "- $i=4 \\rightarrow W_{p}[7,t]=\\sin(t/5^{8/10}), W_{p}[8,t]=\\cos(t/5^{8/10})$\n",
    "- $i=5 \\rightarrow W_{p}[9,t]=\\sin(t/5^{10/10}), W_{p}[10,t]=\\cos(t/5^{10/10})$\n",
    "\n",
    "In this tutorial we assume that position $t$ has range $0\\le t < l_{max}$.  \n",
    "So it takes only two arguments `d` and `l_max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_positional_embedding_matrix(d, l_max):\n",
    "    from math import sin, cos\n",
    "\n",
    "    pos_matrix = {}\n",
    "    for t in range(l_max):\n",
    "        pos_vector = []\n",
    "        for i in range(d):\n",
    "            c = t / (l_max ** (i/d))\n",
    "            if i % 2 == 0:\n",
    "                v = sin(c)\n",
    "            else:\n",
    "                v = cos(c)\n",
    "            pos_vector.append(v)\n",
    "        pos_matrix[t] = pos_vector\n",
    "    return pos_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0],\n",
       " 1: [0.8414709848078965,\n",
       "  0.6589758961876738,\n",
       "  0.66297050495782,\n",
       "  0.8155983039494298,\n",
       "  0.5014773904169323,\n",
       "  0.9016555951500453,\n",
       "  0.3715990270406081,\n",
       "  0.9479277461957519,\n",
       "  0.2724572044052023,\n",
       "  0.9725320837941663],\n",
       " 2: [0.9092974268256817,\n",
       "  -0.13150153648730423,\n",
       "  0.9926597923765998,\n",
       "  0.3304011868103727,\n",
       "  0.8677271400921646,\n",
       "  0.6259656245307648,\n",
       "  0.6899801114404277,\n",
       "  0.7971340240155157,\n",
       "  0.5242991535310988,\n",
       "  0.8916373080180467],\n",
       " 3: [0.1411200080598672,\n",
       "  -0.8322885819012287,\n",
       "  0.8233301012265333,\n",
       "  -0.27664900877859255,\n",
       "  0.9999868910066393,\n",
       "  0.22715522030946758,\n",
       "  0.9095468305649883,\n",
       "  0.5633231714062046,\n",
       "  0.736470428811671,\n",
       "  0.7617596945166576],\n",
       " 4: [-0.7568024953079283,\n",
       "  -0.9654146918029564,\n",
       "  0.24010498558729074,\n",
       "  -0.7816701115085943,\n",
       "  0.8625916771558482,\n",
       "  -0.21633407381161932,\n",
       "  0.998854298357255,\n",
       "  0.27084530448633765,\n",
       "  0.8929172613168825,\n",
       "  0.5900341780993384]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_max = 5\n",
    "pos_matrix = generate_positional_embedding_matrix(embed_dimension, l_max)\n",
    "pos_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positonal embedding of a token is usually added to the token embedding to form a tokens's initial embeding.  \n",
    "For the $t$-th token of a sequence $x$, the embedding is:\n",
    "$$\n",
    "e=W_{e}[:,x[t]]+W_{p}[:,x[t]]\n",
    "$$\n",
    "\n",
    "Define a function that adds embedding vector and positional vector when `position` and `token_id` is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_by_position(token_id, position, embed_matrix, pos_matrix):\n",
    "    embed_vector = embed_matrix[token_id]\n",
    "    pos_vector = pos_matrix[position]\n",
    "    result_vector = []\n",
    "    for e, p in zip(embed_vector, pos_vector):\n",
    "        result_vector.append(e+p)\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 0\n",
    "token_id = 0\n",
    "embed_vecotr = embed_by_position(token_id, position, embed_matrix, pos_matrix)\n",
    "embed_vecotr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained before, in practical, we should map all tokens in sentence.  \n",
    "Define a function that implements mapping to all tokens in sentence.  \n",
    "Use `embed_by_position` that we have defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence, embed_matrix, pos_matrix):\n",
    "    embed_sentence = []\n",
    "    for position, token_id in enumerate(sentence):\n",
    "        token_vector = embed_by_position(token_id, position, embed_matrix, pos_matrix)\n",
    "        embed_sentence.append(token_vector)\n",
    "    return embed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0],\n",
       " [1.8414709848078965,\n",
       "  1.6589758961876737,\n",
       "  1.66297050495782,\n",
       "  1.8155983039494297,\n",
       "  1.5014773904169323,\n",
       "  1.9016555951500453,\n",
       "  1.3715990270406082,\n",
       "  1.9479277461957518,\n",
       "  1.2724572044052023,\n",
       "  1.9725320837941664],\n",
       " [2.909297426825682,\n",
       "  1.8684984635126958,\n",
       "  2.9926597923765996,\n",
       "  2.3304011868103727,\n",
       "  2.8677271400921645,\n",
       "  2.6259656245307648,\n",
       "  2.689980111440428,\n",
       "  2.7971340240155156,\n",
       "  2.524299153531099,\n",
       "  2.891637308018047]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_sentence = sentence_embedding(sample_sentence, embed_matrix, pos_matrix)\n",
    "embed_sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7366666ad7bf0c26a0665a94626a455744caa8a9b32341b6d3aaf8b0adc3aa10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
