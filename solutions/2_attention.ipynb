{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algorithm 3\n",
    "\n",
    "![img](../assets/algorithm_3.png)\n",
    "\n",
    "In Algorithm 3, transformer gets two inputs $e$ and $e_{t}$.  \n",
    "- $e$ : vector representations of the current token. \n",
    "- $e_{t}$ : vector representations of context tokens $t \\in [T]$.  \n",
    "    * context tokens are the tokens that has contextual information (eg. preceding text or the surrounding text) for predicting the current token.\n",
    "\n",
    "And its output is $\\tilde{t}$, vector representation of the token and context combined with those parameters:\n",
    "- $W_{q}, W_{k} \\in \\mathbb{R}^{d_{attn} \\times d_{in}}$\n",
    "- $b_{q}, b_{k} \\in \\mathbb{R}^{d_{attn}}$, the query and key linear projections\n",
    "- $W_{v} \\in \\mathbb{R}^{d_{out} \\times d_{in}},b_{v} \\in \\mathbb{R}^{d_{out}}$, the value linear projection\n",
    "\n",
    "Attention works as follows:\n",
    "1. The token currently being predicted is mapped to a *query* vector $\\bf{q} \\in \\mathbb{R}^{d_{attn}}$.\n",
    "$$\n",
    "\\bf{q} \\leftarrow W_{q}e + b_{q}\n",
    "$$\n",
    "\n",
    "2. The tokens in the context are mapped to *key* vectors $\\bf{k}_{t} \\in \\mathbb{R}^{d_{attn}}$.\n",
    "$$\n",
    "\\forall{t}: \\bf{k}_t \\leftarrow W_{k}e_{t} + b_{q}\n",
    "$$\n",
    "\n",
    "3. The tokens in the context are mapped to *value* vectors $\\bf{v}_{t} \\in \\mathbb{R}^{d_{attn}}$.\n",
    "$$\n",
    "\\forall{t}: \\bf{v}_t \\leftarrow W_{v}e_{t} + b_{q}\n",
    "$$\n",
    "\n",
    "4. The inner products $\\bf{q}^{T}\\bf{k}_{t}$ are interpreted as the degree to which token $t \\in V$ is important for predicting the current token $q$.\n",
    "$$\n",
    "\\forall{t}: \\alpha_t = \\frac{\\exp({\\bf{q}^{T}\\bf{k}_{t}/\\sqrt{d_{attn}}})}{\\sum_{u}{\\exp({\\bf{q}^{T}\\bf{k}_{u}/\\sqrt{d_{attn}}})}}\n",
    "$$\n",
    "\n",
    "5. Derive a distribution over the context tokens, which is then used to combine the value vectors.\n",
    "$$\n",
    "\\text{return } \\tilde{\\bf{v}}=\\sum_{t=1}^{T}{\\alpha_{t}v_{t}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight matrix and bias vectors\n",
    "\n",
    "To implement Algorithm 4, we need weight matrix and bias vector as follows:\n",
    "\n",
    "1. Query\n",
    "    - $W_{q} \\in \\mathbb{R}^{d_{attn} \\times d_{in}}$\n",
    "    - $b_{q} \\in \\mathbb{R}^{d_{attn}}$\n",
    "2. Key\n",
    "    - $W_{k} \\in \\mathbb{R}^{d_{attn} \\times d_{in}}$\n",
    "    - $b_{k} \\in \\mathbb{R}^{d_{attn}}$\n",
    "3. Value\n",
    "    - $W_{v} \\in \\mathbb{R}^{d_{out} \\times d_{in}}$\n",
    "    - $b_{v} \\in \\mathbb{R}^{d_{out}}$\n",
    "\n",
    "We can generalize those matrix as below:\n",
    "\n",
    "- $W \\in \\mathbb{R}^{d_{out_dim} \\times d_{in_dim}}$\n",
    "- $b \\in \\mathbb{R}^{d_{out_dim}}$\n",
    "\n",
    "So, to generate thoes weight and vectors we need two argument `in_dim` and `out_dim`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weight_bias(in_dim, out_dim):\n",
    "    import numpy as np\n",
    "\n",
    "    weights = np.array([[i+1]*out_dim for i in range(out_dim)])\n",
    "    bias = np.array([0] * out_dim)\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_attn = 10\n",
    "d_in = 10\n",
    "d_out = 10\n",
    "\n",
    "query_weights, query_bias = generate_weight_bias(d_attn, d_in)\n",
    "key_weights, key_bias = generate_weight_bias(d_attn, d_in)\n",
    "value_weights, value_bias = generate_weight_bias(d_in, d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Samples\n",
    "Now, we will implemet attention main logics step by step.  \n",
    "Assume that we have vectors like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "current_vector = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "context_vectors = np.array(\n",
    "    [\n",
    "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
    "        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Query Mapping\n",
    "Define a function that implements below:\n",
    "\n",
    "1. The token currently being predicted is mapped to a *query* vector $\\bf{q} \\in \\mathbb{R}^{d_{attn}}$.\n",
    "\n",
    "Function will get three arguments:\n",
    "- Parameters:\n",
    "    - $W_{q}$: `query_weights`\n",
    "    - $b_{q}$: `query_bias`\n",
    "- Currently predicted vector: `current_vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_mapping(current_vector, query_weights, query_bias):\n",
    "    query_vector = query_weights.dot(current_vector) + query_bias\n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10,  20,  30,  40,  50,  60,  70,  80,  90, 100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector = query_mapping(current_vector, query_weights, query_bias)\n",
    "query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Key Maping\n",
    "\n",
    "Define a function that implements below:\n",
    "\n",
    "2. The tokens in the context are mapped to *key* vectors $\\bf{k}_{t} \\in \\mathbb{R}^{d_{attn}}$.\n",
    "$$\n",
    "\\forall{t}: \\bf{k}_t \\leftarrow W_{k}e_{t} + b_{q}\n",
    "$$\n",
    "\n",
    "Function will get three arguments:\n",
    "- Parameters:\n",
    "    - $W_{q}$: `key_weights`\n",
    "    - $b_{q}$: `key_bias`\n",
    "- Context vectors: `context_vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_mapping(context_vectors, key_weights, key_bias):\n",
    "    key_vectors = []\n",
    "    for context_vector in context_vectors:\n",
    "        key_vector = key_weights.dot(context_vector) + key_bias\n",
    "        key_vectors.append(key_vector)\n",
    "    return np.stack(key_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 20,  40,  60,  80, 100, 120, 140, 160, 180, 200],\n",
       "       [ 30,  60,  90, 120, 150, 180, 210, 240, 270, 300],\n",
       "       [ 40,  80, 120, 160, 200, 240, 280, 320, 360, 400]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_vectors = key_mapping(context_vectors, key_weights, key_bias)\n",
    "key_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Value Mapping\n",
    "Define a function that implements below:\n",
    "\n",
    "3. The tokens in the context are mapped to *value* vectors $\\bf{v}_{t} \\in \\mathbb{R}^{d_{attn}}$.\n",
    "$$\n",
    "\\forall{t}: \\bf{v}_t \\leftarrow W_{v}e_{t} + b_{q}\n",
    "$$\n",
    "\n",
    "Function will get three arguments:\n",
    "- Parameters:\n",
    "    - $W_{q}$: `value_weights`\n",
    "    - $b_{q}$: `value_bias`\n",
    "- Context vectors: `context_vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_mapping(context_vectors, value_weights, value_bias):\n",
    "    # solution 1\n",
    "    value_vectors = []\n",
    "    for context_vector in context_vectors:\n",
    "        value_vector = value_weights.dot(context_vector) + value_bias\n",
    "        value_vectors.append(value_vector)\n",
    "    # solution 2\n",
    "    # value_vectors = value_weights.dot(context_vectors.T).T + value_bias\n",
    "    return np.stack(value_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 20,  41,  62,  83, 104, 125, 146, 167, 188, 209],\n",
       "       [ 30,  61,  92, 123, 154, 185, 216, 247, 278, 309],\n",
       "       [ 40,  81, 122, 163, 204, 245, 286, 327, 368, 409]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_vectors = value_mapping(context_vectors, value_weights, value_bias)\n",
    "value_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Softmax\n",
    "Define a function that implements below:\n",
    "\n",
    "4. The inner products $\\bf{q}^{T}\\bf{k}_{t}$ are interpreted as the degree to which token $t \\in V$ is important for predicting the current token $q$.\n",
    "$$\n",
    "\\forall{t}: \\alpha_t = \\frac{\\exp({\\bf{q}^{T}\\bf{k}_{t}/\\sqrt{d_{attn}}})}{\\sum_{u}{\\exp({\\bf{q}^{T}\\bf{k}_{u}/\\sqrt{d_{attn}}})}}\n",
    "$$\n",
    "\n",
    "Function will get three arguments `query_vector`, `key_vectors` and `d_attn`.  \n",
    "Note that result of this function is equal to softmax function.\n",
    "\n",
    "![img](../assets/softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a function that implements inner product between query_vector and one key_vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_product_query_key(query_vector, key_vector, d_attn):\n",
    "    from math import sqrt\n",
    "\n",
    "    alpha = query_vector.dot(key_vector) / sqrt(d_attn)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24349.53798329652"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = inner_product_query_key(query_vector, key_vectors[0], d_attn)\n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, define a function that implements inner product between query_vector and all key_vectors.  \n",
    "Use `inner_product_query_key` function we have defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_product_query_keys(query_vector, key_vectors, d_attn):\n",
    "    # solution 1\n",
    "    alphas = []\n",
    "    for key_vector in key_vectors:\n",
    "        alpha = inner_product_query_key(query_vector, key_vector, d_attn)\n",
    "        alphas.append(alpha)\n",
    "    return np.array(alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of `alphas` should be eqaul to length of `context_vectors`.  \n",
    "In this tutorial it should be 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24349.5379833 , 36524.30697494, 48699.07596659])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = inner_product_query_keys(query_vector, key_vectors, d_attn)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, make an softmax function using `alphas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(alphas):\n",
    "    scores = alphas / alphas.sum()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum of score should be equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = softmax(alphas)\n",
    "sum(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Final output\n",
    "Define a function that implements below:\n",
    "\n",
    "5. Derive a distribution over the context tokens, which is then used to combine the value vectors.\n",
    "$$\n",
    "\\text{return } \\tilde{\\bf{v}}=\\sum_{t=1}^{T}{\\alpha_{t}v_{t}}\n",
    "$$\n",
    "\n",
    "Function will get two arguments `value_vectors` and `scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_value_score(value_vectors, scores):\n",
    "    outputs = scores.dot(value_vectors)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of `outputs` should be equal to `d_out`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = combine_value_score(value_vectors, scores)\n",
    "len(outputs), d_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7366666ad7bf0c26a0665a94626a455744caa8a9b32341b6d3aaf8b0adc3aa10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
