{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algorithm 3\n",
    "\n",
    "![img](../assets/algorithm_3.png)\n",
    "\n",
    "In Algorithm 3, attention gets two inputs $e$ and $e_{t}$.  \n",
    "- $e$ : vector representations of the current token. \n",
    "- $e_{t}$ : vector representations of context tokens $t \\in [T]$.  \n",
    "    * context tokens are the tokens that has contextual information (eg. preceding text or the surrounding text) for predicting the current token.\n",
    "\n",
    "And its output is $\\tilde{t}$, vector representation of the token and context combined with those parameters:\n",
    "- $W_{q}, W_{k} \\in \\mathbb{R}^{d_{attn} \\times d_{in}}$\n",
    "- $b_{q}, b_{k} \\in \\mathbb{R}^{d_{attn}}$, the query and key linear projections\n",
    "- $W_{v} \\in \\mathbb{R}^{d_{out} \\times d_{in}},b_{v} \\in \\mathbb{R}^{d_{out}}$, the value linear projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention works as follows:\n",
    "1. The token currently being predicted is mapped to a *query* vector $\\bf{q} \\in \\mathbb{R}^{d_{attn}}$.\n",
    "$$\n",
    "\\bf{q} \\leftarrow W_{q}e + b_{q}\n",
    "$$\n",
    "\n",
    "2. The tokens in the context are mapped to *key* vectors $\\bf{k}_{t} \\in \\mathbb{R}^{d_{attn}}$.\n",
    "$$\n",
    "\\forall{t}: \\bf{k}_t \\leftarrow W_{k}e_{t} + b_{q}\n",
    "$$\n",
    "\n",
    "3. The tokens in the context are mapped to *value* vectors $\\bf{v}_{t} \\in \\mathbb{R}^{d_{attn}}$.\n",
    "$$\n",
    "\\forall{t}: \\bf{v}_t \\leftarrow W_{v}e_{t} + b_{q}\n",
    "$$\n",
    "\n",
    "4. The inner products $\\bf{q}^{T}\\bf{k}_{t}$ are interpreted as the degree to which token $t \\in V$ is important for predicting the current token $q$.\n",
    "$$\n",
    "\\forall{t}: \\alpha_t = \\frac{\\exp({\\bf{q}^{T}\\bf{k}_{t}/\\sqrt{d_{attn}}})}{\\sum_{u}{\\exp({\\bf{q}^{T}\\bf{k}_{u}/\\sqrt{d_{attn}}})}}\n",
    "$$\n",
    "\n",
    "5. Derive a distribution over the context tokens, which is then used to combine the value vectors.\n",
    "$$\n",
    "\\text{return } \\tilde{\\bf{v}}=\\sum_{t=1}^{T}{\\alpha_{t}v_{t}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight matrix and bias vectors\n",
    "\n",
    "To implement Algorithm 3, we need weight matrix and bias vector as follows:\n",
    "\n",
    "1. Query\n",
    "    - $W_{q} \\in \\mathbb{R}^{d_{attn} \\times d_{in}}$\n",
    "    - $b_{q} \\in \\mathbb{R}^{d_{attn}}$\n",
    "2. Key\n",
    "    - $W_{k} \\in \\mathbb{R}^{d_{attn} \\times d_{in}}$\n",
    "    - $b_{k} \\in \\mathbb{R}^{d_{attn}}$\n",
    "3. Value\n",
    "    - $W_{v} \\in \\mathbb{R}^{d_{out} \\times d_{in}}$\n",
    "    - $b_{v} \\in \\mathbb{R}^{d_{out}}$\n",
    "\n",
    "We can generalize those matrix as below:\n",
    "\n",
    "- $W \\in \\mathbb{R}^{d_{out_dim} \\times d_{in_dim}}$\n",
    "- $b \\in \\mathbb{R}^{d_{out_dim}}$\n",
    "\n",
    "So, to generate thoes weight and vectors we need two argument `in_dim` and `out_dim`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weight_bias(in_dim, out_dim):\n",
    "    import numpy as np\n",
    "\n",
    "    weights = np.array([[i + 1] * in_dim for i in range(out_dim)])\n",
    "    bias = np.array([i+1 for i in range(out_dim)])\n",
    "    return weights, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_attn = 10\n",
    "d_in = 10\n",
    "d_out = 10\n",
    "\n",
    "query_weights, query_bias = generate_weight_bias(d_attn, d_in)\n",
    "key_weights, key_bias = generate_weight_bias(d_attn, d_in)\n",
    "value_weights, value_bias = generate_weight_bias(d_in, d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "### Input Samples\n",
    "Now, we will implemet attention main logics step by step.  \n",
    "Assume that we have vectors like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "current_vector = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "context_vectors = np.array(\n",
    "    [\n",
    "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
    "        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Query Mapping\n",
    "Define a function that implements below:\n",
    "\n",
    "1. The token currently being predicted is mapped to a *query* vector $\\bf{q} \\in \\mathbb{R}^{d_{attn}}$.\n",
    "\n",
    "Function will get three arguments:\n",
    "- Parameters:\n",
    "    - $W_{q}$: `query_weights`\n",
    "    - $b_{q}$: `query_bias`\n",
    "- Currently predicted vector: `current_vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_mapping(current_vector, query_weights, query_bias):\n",
    "    query_vector = query_weights.dot(current_vector) + query_bias\n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11,  22,  33,  44,  55,  66,  77,  88,  99, 110])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector = query_mapping(current_vector, query_weights, query_bias)\n",
    "query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Key Maping\n",
    "\n",
    "Define a function that implements below:\n",
    "\n",
    "2. The tokens in the context are mapped to *key* vectors $\\bf{k}_{t} \\in \\mathbb{R}^{d_{attn}}$.\n",
    "$$\n",
    "\\forall{t}: \\bf{k}_t \\leftarrow W_{k}e_{t} + b_{q}\n",
    "$$\n",
    "\n",
    "Function will get three arguments:\n",
    "- Parameters:\n",
    "    - $W_{q}$: `key_weights`\n",
    "    - $b_{q}$: `key_bias`\n",
    "- Context vectors: `context_vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_mapping(context_vectors, key_weights, key_bias):\n",
    "    key_vectors = []\n",
    "    for context_vector in context_vectors:\n",
    "        key_vector = key_weights.dot(context_vector) + key_bias\n",
    "        key_vectors.append(key_vector)\n",
    "    return np.stack(key_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 21,  42,  63,  84, 105, 126, 147, 168, 189, 210],\n",
       "       [ 31,  62,  93, 124, 155, 186, 217, 248, 279, 310],\n",
       "       [ 41,  82, 123, 164, 205, 246, 287, 328, 369, 410]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_vectors = key_mapping(context_vectors, key_weights, key_bias)\n",
    "key_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Value Mapping\n",
    "Define a function that implements below:\n",
    "\n",
    "3. The tokens in the context are mapped to *value* vectors $\\bf{v}_{t} \\in \\mathbb{R}^{d_{attn}}$.\n",
    "$$\n",
    "\\forall{t}: \\bf{v}_t \\leftarrow W_{v}e_{t} + b_{q}\n",
    "$$\n",
    "\n",
    "Function will get three arguments:\n",
    "- Parameters:\n",
    "    - $W_{q}$: `value_weights`\n",
    "    - $b_{q}$: `value_bias`\n",
    "- Context vectors: `context_vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_mapping(context_vectors, value_weights, value_bias):\n",
    "    # solution 1\n",
    "    value_vectors = []\n",
    "    for context_vector in context_vectors:\n",
    "        value_vector = value_weights.dot(context_vector) + value_bias\n",
    "        value_vectors.append(value_vector)\n",
    "    # solution 2\n",
    "    # value_vectors = value_weights.dot(context_vectors.T).T + value_bias\n",
    "    return np.stack(value_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 21,  42,  63,  84, 105, 126, 147, 168, 189, 210],\n",
       "       [ 31,  62,  93, 124, 155, 186, 217, 248, 279, 310],\n",
       "       [ 41,  82, 123, 164, 205, 246, 287, 328, 369, 410]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_vectors = value_mapping(context_vectors, value_weights, value_bias)\n",
    "value_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Softmax\n",
    "Define a function that implements below:\n",
    "\n",
    "4. The inner products $\\bf{q}^{T}\\bf{k}_{t}$ are interpreted as the degree to which token $t \\in V$ is important for predicting the current token $q$.\n",
    "$$\n",
    "\\forall{t}: \\alpha_t = \\frac{\\exp({\\bf{q}^{T}\\bf{k}_{t}/\\sqrt{d_{attn}}})}{\\sum_{u}{\\exp({\\bf{q}^{T}\\bf{k}_{u}/\\sqrt{d_{attn}}})}}\n",
    "$$\n",
    "\n",
    "Function will get three arguments `query_vector`, `key_vectors` and `d_attn`.  \n",
    "Note that result of this function is equal to softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a function that implements inner product between query_vector and one key_vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_product_query_key(query_vector, key_vector, d_attn):\n",
    "    from math import sqrt\n",
    "\n",
    "    alpha = query_vector.dot(key_vector) / sqrt(d_attn)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28123.71637070748"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = inner_product_query_key(query_vector, key_vectors[0], d_attn)\n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, define a function that implements inner product between query_vector and all key_vectors.  \n",
    "Use `inner_product_query_key` function we have defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_product_query_keys(query_vector, key_vectors, d_attn):\n",
    "    alphas = []\n",
    "    for key_vector in key_vectors:\n",
    "        alpha = inner_product_query_key(query_vector, key_vector, d_attn)\n",
    "        alphas.append(alpha)\n",
    "    return np.array(alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of `alphas` should be eqaul to length of `context_vectors`.  \n",
    "In this tutorial it should be 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28123.71637071, 41515.96226152, 54908.20815233])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = inner_product_query_keys(query_vector, key_vectors, d_attn)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, make an softmax function using `alphas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(alphas):\n",
    "    scores = alphas / alphas.sum()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum of score should be equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = softmax(alphas)\n",
    "sum(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Final output\n",
    "Define a function that implements below:\n",
    "\n",
    "5. Derive a distribution over the context tokens, which is then used to combine the value vectors.\n",
    "$$\n",
    "\\text{return } \\tilde{\\bf{v}}=\\sum_{t=1}^{T}{\\alpha_{t}v_{t}}\n",
    "$$\n",
    "\n",
    "Function will get two arguments `value_vectors` and `scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_value_score(value_vectors, scores):\n",
    "    outputs = scores.dot(value_vectors)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of `outputs` should be equal to `d_out`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = combine_value_score(value_vectors, scores)\n",
    "len(outputs), d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate\n",
    "\n",
    "Now, aggregate all functions we defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(\n",
    "    current_vector, context_vectors, query_weights, key_weights, value_weights, query_bias, key_bias, value_bias, d_attn\n",
    "):\n",
    "    # 1. Query Mapping\n",
    "    query_vector = query_mapping(current_vector, query_weights, query_bias)\n",
    "    # 2. Key Mapping\n",
    "    key_vectors = key_mapping(context_vectors, key_weights, key_bias)\n",
    "    # 3. Value Mapping\n",
    "    value_vectors = value_mapping(context_vectors, value_weights, value_bias)\n",
    "    # 4. Softmax\n",
    "    alphas = inner_product_query_keys(query_vector, key_vectors, d_attn)\n",
    "    scores = softmax(alphas)\n",
    "    # 5. final output\n",
    "    outputs = combine_value_score(value_vectors, scores)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 33.15053763,  66.30107527,  99.4516129 , 132.60215054,\n",
       "       165.75268817, 198.90322581, 232.05376344, 265.20430108,\n",
       "       298.35483871, 331.50537634])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_hidden = attention(current_vector, context_vectors, query_weights, key_weights, value_weights, query_bias, key_bias, value_bias, d_attn)\n",
    "attn_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask Attention\n",
    "\n",
    "> There are many ways the basic attention mechanism is used in transformers.\n",
    "> - Bidrectional / unmasked self-attention\n",
    "> - Unidrectional / masked self-attention\n",
    "> - Cross-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Algorithm 4\n",
    "\n",
    "![img](../assets/algorithm_4.png)\n",
    "\n",
    "In Algorithm 4, masked attention gets two inputs $\\bf{X} \\in \\mathbb{R}^{d_{x} \\times l_{X}}$ and $\\bf{Z} \\in \\mathbb{R}^{d_{z} \\times l_{z}}$.\n",
    "- $\\bf{X}$ : vector representation of primary\n",
    "- $\\bf{Z}$ : vector representation of context sequence\n",
    "\n",
    "And its output $\\tilde{\\bf{V}} \\in \\mathbb{R}^{d_{out} \\times l_{z}}$, updated representations of tokens in $\\bf{X}$, folding in infromation from tokens in $\\bf{Z}$ with those parameters and hyperparameters:\n",
    "\n",
    "**Parameters**\n",
    "- $W_{q} \\in \\mathbb{R}^{d_{attn} \\times d_{X}}, b_{q} \\in \\mathbb{R}^{d_{attn}}$\n",
    "- $W_{k} \\in \\mathbb{R}^{d_{attn} \\times d_{Z}}, b_{k} \\in \\mathbb{R}^{d_{attn}}$\n",
    "- $W_{v} \\in \\mathbb{R}^{d_{out} \\times d_{Z}}, b_{v} \\in \\mathbb{R}^{d_{out}}$\n",
    "\n",
    "**Hyper Parameters**\n",
    "- $\\text{Mask} \\in \\{0,1\\}^{l_{Z}\\times l_{X}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Softmax function with matrix form\n",
    "\n",
    "![img](../assets/softmax.png)\n",
    "\n",
    "- Mask function with matrix form\n",
    "\n",
    "![img](../assets/mask.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masked Attention works as follows:  \n",
    "1. $\\bf{Q} \\leftarrow W_{q}\\bf{X} + b_{q}\\bf{1}^T$\n",
    "2. $\\bf{K} \\leftarrow W_{k}\\bf{X} + b_{k}\\bf{1}^T$\n",
    "3. $\\bf{V} \\leftarrow W_{v}\\bf{X} + b_{v}\\bf{1}^T$\n",
    "4. $\\bf{S} \\leftarrow \\bf{K}^{T}Q$\n",
    "5. $\\forall{t_{Z},t_{X}} \\text{ if }\\neg\\text{Mask}[t_{Z}, t_{X}]$ then $S[t_{Z}, t_{X}] \\leftarrow - \\infty$\n",
    "6. $\\text{return } \\tilde{\\bf{V}}=\\bf{V} \\sdot \\text{softmax } (\\bf{S}/\\sqrt{d_{attn}})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "### Input Sample\n",
    "Now, we will implemet attention main logics step by step.  \n",
    "Assume that we have matrix, which row is words and column is embed, like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "current_matrix = np.array(\n",
    "    [\n",
    "        [1] * 10,\n",
    "        [2] * 10,\n",
    "        [3] * 10,\n",
    "    ]\n",
    ")\n",
    "context_matrix = np.array(\n",
    "    [\n",
    "        [\n",
    "            [2] * 10,\n",
    "            [3] * 10,\n",
    "            [4] * 10,\n",
    "        ],\n",
    "        [\n",
    "            [3] * 10,\n",
    "            [4] * 10,\n",
    "            [5] * 10,\n",
    "        ],\n",
    "        [\n",
    "            [4] * 10,\n",
    "            [5] * 10,\n",
    "            [6] * 10,\n",
    "        ],\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]],\n",
       "\n",
       "       [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]],\n",
       "\n",
       "       [[4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       "        [5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       "        [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Query Mapping\n",
    "Define a function that implements below:\n",
    "\n",
    "$$\n",
    "\\bf{Q} \\leftarrow W_{q}\\bf{X} + b_{q}\\bf{1}^T\n",
    "$$\n",
    "\n",
    "Function will get three arguments:\n",
    "- Parameters:\n",
    "    - $W_{q}$: `query_weights`\n",
    "    - $b_{q}$: `query_bias`\n",
    "- Currently predicted matrix: `current_matrix`\n",
    "\n",
    "*Hint*: use `query_mapping` for each token and concat results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_query_mapping(current_matrix, query_weights, query_bias):\n",
    "    query_matrix = []\n",
    "    for current_vector in current_matrix:\n",
    "        query_vector = query_mapping(current_vector, query_weights, query_bias)\n",
    "        query_matrix.append(query_vector)\n",
    "    return np.stack(query_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11,  22,  33,  44,  55,  66,  77,  88,  99, 110],\n",
       "       [ 21,  42,  63,  84, 105, 126, 147, 168, 189, 210],\n",
       "       [ 31,  62,  93, 124, 155, 186, 217, 248, 279, 310]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_matrix = masked_query_mapping(current_matrix, query_weights, query_bias)\n",
    "query_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`query_matrix[0]` and `query_vector` should be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 11,  22,  33,  44,  55,  66,  77,  88,  99, 110]),\n",
       " array([ 11,  22,  33,  44,  55,  66,  77,  88,  99, 110]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_matrix[0], query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Key Maping\n",
    "\n",
    "Define a function that implements below:\n",
    "\n",
    "$$\n",
    "\\bf{K} \\leftarrow W_{k}\\bf{X} + b_{k}\\bf{1}^T\n",
    "$$\n",
    "\n",
    "Function will get three arguments:\n",
    "- Parameters:\n",
    "    - $W_{k}$: `key_weights`\n",
    "    - $b_{k}$: `key_bias`\n",
    "- Context matrix: `context_matrix`\n",
    "\n",
    "*Hint*: use `key_mapping` for each token and concat results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_key_mapping(context_matrix, key_weights, key_bias):\n",
    "    key_matrix = []\n",
    "    for context_vector in context_matrix:\n",
    "        key_vector = key_mapping(context_vector, key_weights, key_bias)\n",
    "        key_matrix.append(key_vector)\n",
    "    return np.stack(key_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 21,  42,  63,  84, 105, 126, 147, 168, 189, 210],\n",
       "        [ 31,  62,  93, 124, 155, 186, 217, 248, 279, 310],\n",
       "        [ 41,  82, 123, 164, 205, 246, 287, 328, 369, 410]],\n",
       "\n",
       "       [[ 31,  62,  93, 124, 155, 186, 217, 248, 279, 310],\n",
       "        [ 41,  82, 123, 164, 205, 246, 287, 328, 369, 410],\n",
       "        [ 51, 102, 153, 204, 255, 306, 357, 408, 459, 510]],\n",
       "\n",
       "       [[ 41,  82, 123, 164, 205, 246, 287, 328, 369, 410],\n",
       "        [ 51, 102, 153, 204, 255, 306, 357, 408, 459, 510],\n",
       "        [ 61, 122, 183, 244, 305, 366, 427, 488, 549, 610]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_matrix = masked_key_mapping(context_matrix, key_weights, key_bias)\n",
    "key_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`key_matrix[0]` and `key_vectors` should be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 21,  42,  63,  84, 105, 126, 147, 168, 189, 210],\n",
       "        [ 31,  62,  93, 124, 155, 186, 217, 248, 279, 310],\n",
       "        [ 41,  82, 123, 164, 205, 246, 287, 328, 369, 410]]),\n",
       " array([[ 21,  42,  63,  84, 105, 126, 147, 168, 189, 210],\n",
       "        [ 31,  62,  93, 124, 155, 186, 217, 248, 279, 310],\n",
       "        [ 41,  82, 123, 164, 205, 246, 287, 328, 369, 410]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_matrix[0], key_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Value Mapping\n",
    "\n",
    "Define a function that implements below:\n",
    "\n",
    "$$\n",
    "\\bf{V} \\leftarrow W_{v}\\bf{X} + b_{v}\\bf{1}^T\n",
    "$$\n",
    "\n",
    "Function will get three arguments:\n",
    "- Parameters:\n",
    "    - $W_{v}$: `value_weights`\n",
    "    - $b_{v}$: `value_bias`\n",
    "- Context matrix: `context_matrix`\n",
    "\n",
    "*Hint*: use `value_mapping` for each token and concat results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_value_mapping(context_matrix, value_weights, value_bias):\n",
    "    value_matrix = []\n",
    "    for context_vector in context_matrix:\n",
    "        value_vector = value_mapping(context_vector, value_weights, value_bias)\n",
    "        value_matrix.append(value_vector)\n",
    "    return np.stack(value_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 21,  42,  63,  84, 105, 126, 147, 168, 189, 210],\n",
       "        [ 31,  62,  93, 124, 155, 186, 217, 248, 279, 310],\n",
       "        [ 41,  82, 123, 164, 205, 246, 287, 328, 369, 410]],\n",
       "\n",
       "       [[ 31,  62,  93, 124, 155, 186, 217, 248, 279, 310],\n",
       "        [ 41,  82, 123, 164, 205, 246, 287, 328, 369, 410],\n",
       "        [ 51, 102, 153, 204, 255, 306, 357, 408, 459, 510]],\n",
       "\n",
       "       [[ 41,  82, 123, 164, 205, 246, 287, 328, 369, 410],\n",
       "        [ 51, 102, 153, 204, 255, 306, 357, 408, 459, 510],\n",
       "        [ 61, 122, 183, 244, 305, 366, 427, 488, 549, 610]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_matrix = masked_value_mapping(context_matrix, value_weights, value_bias)\n",
    "value_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`value_matrix[0]` and `value_vectors` should be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 21,  42,  63,  84, 105, 126, 147, 168, 189, 210],\n",
       "        [ 31,  62,  93, 124, 155, 186, 217, 248, 279, 310],\n",
       "        [ 41,  82, 123, 164, 205, 246, 287, 328, 369, 410]]),\n",
       " array([[ 21,  42,  63,  84, 105, 126, 147, 168, 189, 210],\n",
       "        [ 31,  62,  93, 124, 155, 186, 217, 248, 279, 310],\n",
       "        [ 41,  82, 123, 164, 205, 246, 287, 328, 369, 410]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_matrix[0], value_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calucate Score\n",
    "Define a function that implements below:\n",
    "\n",
    "$$\n",
    "\\bf{S} \\leftarrow \\bf{K}^{T}Q\n",
    "$$\n",
    "\n",
    "Function will get two arguments `query_matrix`, `key_matrix`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a function that implements inner product between query_matrix and one key_matrix.  \n",
    "*Hint*: use `inner_product_query_keys` for each token with `d_attn=1` and concat results. `d_attn` will be calucated after in masked attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_product_query_key_matrix(query_matrix, key_matrix):\n",
    "    alpha_matrix = []\n",
    "    for query_vector, key_vectors in zip(query_matrix, key_matrix):\n",
    "        alphas = inner_product_query_keys(query_vector, key_vectors, d_attn=1)\n",
    "        alpha_matrix.append(alphas)\n",
    "    return np.stack(alpha_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 88935., 131285., 173635.],\n",
       "       [250635., 331485., 412335.],\n",
       "       [489335., 608685., 728035.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_matrix = inner_product_query_key_matrix(query_matrix, key_matrix)\n",
    "alpha_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that there is a mask matrix below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_matrix = np.array([\n",
    "    [0]*len(context_matrix[0]),\n",
    "    [0]*len(context_matrix[0]),\n",
    "    [0]*len(context_matrix[0]),\n",
    "])\n",
    "mask_matrix[1, 1] = 1\n",
    "mask_matrix[2, 2] = 1\n",
    "mask_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that implements below:\n",
    "\n",
    "$$\n",
    "\\forall{t_{Z},t_{X}} \\text{ if }\\neg\\text{Mask}[t_{Z}, t_{X}] \\text{ then }S[t_{Z}, t_{X}] \\leftarrow - \\infty\n",
    "$$\n",
    "\n",
    "Function will get two arguments `alpha_matrix` and `mask_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_score(alpha_matrix, mask_matrix):\n",
    "    masked_alpha = alpha_matrix.copy()\n",
    "    masked_alpha[mask_matrix==1] = 1\n",
    "    return masked_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.89350e+04, 1.31285e+05, 1.73635e+05],\n",
       "       [2.50635e+05, 1.00000e+00, 4.12335e+05],\n",
       "       [4.89335e+05, 6.08685e+05, 1.00000e+00]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_alpha = mask_score(alpha_matrix, mask_matrix)\n",
    "masked_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Final output\n",
    "Define a function that implements below:\n",
    "\n",
    "$$\n",
    "\\text{return } \\tilde{\\bf{V}}=\\bf{V} \\sdot \\text{softmax } (\\bf{S}/\\sqrt{d_{attn}})\n",
    "$$\n",
    "\n",
    "Function will get three arguments `value_matrix`, `alpha_matrix` and `d_attn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, calucate softmax with `alpha_matrix`.  \n",
    "*Hint*: use `softmax` for each token and concat results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(alpha_matrix, d_attn):\n",
    "    scores = []\n",
    "    for alphas in alpha_matrix:\n",
    "        score = softmax(alphas/d_attn)\n",
    "        scores.append(score)\n",
    "    return np.stack(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22580645, 0.33333333, 0.44086022],\n",
       "       [0.25203252, 0.33333333, 0.41463415],\n",
       "       [0.26797386, 0.33333333, 0.39869281]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_scores = masked_softmax(alpha_matrix, d_attn)\n",
    "masked_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`masked_scores[0]` and `scores` should be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.22580645, 0.33333333, 0.44086022]),\n",
       " array([0.22580645, 0.33333333, 0.44086022]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_scores[0], scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next combine `value_matrix` and `masked_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_value_masked_score(value_matrix, masked_scores):\n",
    "    outputs = []\n",
    "    for value_vectors, scores in zip(value_matrix, masked_scores):\n",
    "        output = combine_value_score(value_vectors, scores)\n",
    "        outputs.append(output)\n",
    "    return np.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 33.15053763,  66.30107527,  99.4516129 , 132.60215054,\n",
       "        165.75268817, 198.90322581, 232.05376344, 265.20430108,\n",
       "        298.35483871, 331.50537634],\n",
       "       [ 42.62601626,  85.25203252, 127.87804878, 170.50406504,\n",
       "        213.1300813 , 255.75609756, 298.38211382, 341.00813008,\n",
       "        383.63414634, 426.2601626 ],\n",
       "       [ 52.30718954, 104.61437908, 156.92156863, 209.22875817,\n",
       "        261.53594771, 313.84313725, 366.1503268 , 418.45751634,\n",
       "        470.76470588, 523.07189542]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_outputs = combine_value_masked_score(value_matrix, masked_scores)\n",
    "masked_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`masked_outputs[0]` and `outputs` should be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 33.15053763,  66.30107527,  99.4516129 , 132.60215054,\n",
       "        165.75268817, 198.90322581, 232.05376344, 265.20430108,\n",
       "        298.35483871, 331.50537634]),\n",
       " array([ 33.15053763,  66.30107527,  99.4516129 , 132.60215054,\n",
       "        165.75268817, 198.90322581, 232.05376344, 265.20430108,\n",
       "        298.35483871, 331.50537634]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_outputs[0], outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate\n",
    "\n",
    "Now, aggregate all functions we defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_attention(\n",
    "    current_matrix,\n",
    "    context_matrix,\n",
    "    mask_matrix,\n",
    "    query_weights,\n",
    "    key_weights,\n",
    "    value_weights,\n",
    "    query_bias,\n",
    "    key_bias,\n",
    "    value_bias,\n",
    "    d_attn,\n",
    "):\n",
    "    # 1. Query Mapping\n",
    "    query_matrix = masked_query_mapping(current_matrix, query_weights, query_bias)\n",
    "    # 2. Key Mapping\n",
    "    key_matrix = masked_key_mapping(context_matrix, key_weights, key_bias)\n",
    "    # 3. Value Mapping\n",
    "    value_matrix = masked_value_mapping(context_matrix, value_weights, value_bias)\n",
    "    # 4. Calucate Score\n",
    "    alpha_matrix = inner_product_query_key_matrix(query_matrix, key_matrix)\n",
    "    # 5. Maksing\n",
    "    masked_alpha = mask_score(alpha_matrix, mask_matrix)\n",
    "    # 6. Final output\n",
    "    masked_scores = masked_softmax(alpha_matrix, d_attn)\n",
    "    masked_outputs = combine_value_masked_score(value_matrix, masked_scores)\n",
    "    return masked_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 33.15053763,  66.30107527,  99.4516129 , 132.60215054,\n",
       "        165.75268817, 198.90322581, 232.05376344, 265.20430108,\n",
       "        298.35483871, 331.50537634],\n",
       "       [ 42.62601626,  85.25203252, 127.87804878, 170.50406504,\n",
       "        213.1300813 , 255.75609756, 298.38211382, 341.00813008,\n",
       "        383.63414634, 426.2601626 ],\n",
       "       [ 52.30718954, 104.61437908, 156.92156863, 209.22875817,\n",
       "        261.53594771, 313.84313725, 366.1503268 , 418.45751634,\n",
       "        470.76470588, 523.07189542]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attn_hidden = masked_attention(\n",
    "    current_matrix,\n",
    "    context_matrix,\n",
    "    mask_matrix,\n",
    "    query_weights,\n",
    "    key_weights,\n",
    "    value_weights,\n",
    "    query_bias,\n",
    "    key_bias,\n",
    "    value_bias,\n",
    "    d_attn,\n",
    ")\n",
    "masked_attn_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention\n",
    "\n",
    "- Algorithm 5\n",
    "\n",
    "![img](../assets/algorithm_5.png)\n",
    "\n",
    "In Algorithm 5, multi-head attention gets two inputs $\\bf{X} \\in \\mathbb{R}^{d_{x} \\times l_{X}}$ and $\\bf{Z} \\in \\mathbb{R}^{d_{z} \\times l_{z}}$.\n",
    "- $\\bf{X}$ : vector representation of primary\n",
    "- $\\bf{Z}$ : vector representation of context sequence\n",
    "\n",
    "And its output $\\tilde{\\bf{V}} \\in \\mathbb{R}^{d_{out} \\times l_{z}}$, updated representations of tokens in $\\bf{X}$, folding in infromation from tokens in $\\bf{Z}$ with those parameters and hyperparameters:\n",
    "\n",
    "**Parameters**\n",
    "- For $h \\in [H]$\n",
    "    - $W_{q}^{h} \\in \\mathbb{R}^{d_{attn} \\times d_{X}}, b_{q}^{h} \\in \\mathbb{R}^{d_{attn}}$\n",
    "    - $W_{k}^{h} \\in \\mathbb{R}^{d_{attn} \\times d_{Z}}, b_{k}^{h} \\in \\mathbb{R}^{d_{attn}}$\n",
    "    - $W_{v}^{h} \\in \\mathbb{R}^{d_{out} \\times d_{Z}}, b_{v}^{h} \\in \\mathbb{R}^{d_{out}}$\n",
    "\n",
    "**Hyper Parameters**\n",
    "- $H$, Number of attention heads\n",
    "- $\\text{Mask} \\in \\{0,1\\}^{l_{Z}\\times l_{X}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Head Attention works as follows:  \n",
    "1. For $h \\in [H]$  \n",
    "        $\\bf{Y}^{h} \\gets \\text{Attention}(\\bf{X},\\bf{Z}|\\bf{W}_{qkv}^{h},\\text{Mask})$\n",
    "2. $\\bf{Y} \\gets [\\bf{Y}^{1}; \\bf{Y}^{2}; ...;\\bf{Y}^{H}]$\n",
    "3. $\\text{return } \\tilde{\\bf{V}}=\\bf{W}_{0}\\bf{Y}+\\bf{b}_{0}\\bf{1}^T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head Weight matrix and bias\n",
    "\n",
    "To implement Algorithm 5, we need $h$ set of weight matrix and bias vector as follows:\n",
    "\n",
    "- For $h \\in [H]$:\n",
    "    1. Query\n",
    "        - $W_{q}^{h} \\in \\mathbb{R}^{d_{attn} \\times d_{X}}$\n",
    "        - $b_{q}^{h} \\in \\mathbb{R}^{d_{attn}}$\n",
    "    2. Key\n",
    "        - $W_{k}^{h} \\in \\mathbb{R}^{d_{attn} \\times d_{X}}$\n",
    "        - $b_{k}^{h} \\in \\mathbb{R}^{d_{attn}}$\n",
    "    3. Value\n",
    "        - $W_{v}^{h} \\in \\mathbb{R}^{d_{mid} \\times d_{Z}}$\n",
    "        - $b_{v}^{h} \\in \\mathbb{R}^{d_{mid}}$\n",
    "- Combine weight and bias\n",
    "    - $W_{o} \\in \\mathbb{R}^{d_{out} \\times Hd_{mid}}$\n",
    "    - $b_{o} \\in \\mathbb{R}^{d_{out}}$\n",
    "\n",
    "To generate thoes set of weight and vectors we need four argument `n_head`, `d_attn`, `d_in` and `d_out`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a function thate generates a set of qeury, key, value weights and bias.\n",
    "\n",
    "*Hint*: use `generate_weight_bias` for haad and concat results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_weights(d_attn, d_in, d_out):\n",
    "    query_weights, query_bias = generate_weight_bias(d_attn, d_in)\n",
    "    query_dict = {\n",
    "        \"weight\": query_weights,\n",
    "        \"bias\": query_bias\n",
    "    }\n",
    "    key_weights, key_bias = generate_weight_bias(d_attn, d_in)\n",
    "    key_dict = {\n",
    "        \"weight\": key_weights,\n",
    "        \"bias\": key_bias\n",
    "    }\n",
    "    value_weights, value_bias = generate_weight_bias(d_in, d_out)\n",
    "    value_dict = {\n",
    "        \"weight\": value_weights,\n",
    "        \"bias\": value_bias\n",
    "    }\n",
    "    return query_dict, key_dict, value_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a function that generates $h$ sets of qeury, key, value weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_weights(n_head, d_attn, d_in, d_out):\n",
    "    query_weight_dict = {\"weight\":[], \"bias\": []}\n",
    "    key_weight_dict = {\"weight\":[], \"bias\": []}\n",
    "    value_weight_dict = {\"weight\":[], \"bias\": []}\n",
    "    for i in range(n_head):\n",
    "        query_dict, key_dict, value_dict = head_weights(d_attn, d_in, d_out)\n",
    "        query_weight_dict[\"weight\"].append(query_dict[\"weight\"])\n",
    "        query_weight_dict[\"bias\"].append(query_dict[\"bias\"])\n",
    "        key_weight_dict[\"weight\"].append(key_dict[\"weight\"])\n",
    "        key_weight_dict[\"bias\"].append(key_dict[\"bias\"])\n",
    "        value_weight_dict[\"weight\"].append(value_dict[\"weight\"])\n",
    "        value_weight_dict[\"bias\"].append(value_dict[\"bias\"])\n",
    "    query_weight_dict[\"weight\"] = np.stack(query_weight_dict[\"weight\"])\n",
    "    query_weight_dict[\"bias\"] = np.stack(query_weight_dict[\"bias\"])\n",
    "    key_weight_dict[\"weight\"] = np.stack(key_weight_dict[\"weight\"])\n",
    "    key_weight_dict[\"bias\"] = np.stack(key_weight_dict[\"bias\"])\n",
    "    value_weight_dict[\"weight\"] = np.stack(value_weight_dict[\"weight\"])\n",
    "    value_weight_dict[\"bias\"] = np.stack(value_weight_dict[\"bias\"])\n",
    "    return query_weight_dict, key_weight_dict, value_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'weight': array([[[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "          [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "          [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
       "          [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
       "          [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
       "          [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
       "          [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
       "          [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\n",
       "          [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
       "          [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]],\n",
       "  \n",
       "         [[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "          [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "          [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
       "          [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
       "          [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
       "          [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
       "          [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
       "          [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\n",
       "          [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
       "          [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]],\n",
       "  \n",
       "         [[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "          [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "          [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
       "          [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
       "          [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
       "          [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
       "          [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
       "          [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\n",
       "          [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
       "          [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]],\n",
       "  \n",
       "         [[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "          [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "          [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
       "          [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
       "          [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
       "          [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
       "          [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
       "          [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\n",
       "          [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
       "          [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]]]),\n",
       "  'bias': array([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
       "         [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
       "         [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
       "         [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])},\n",
       " {'weight': array([[[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "          [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "          [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
       "          [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
       "          [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
       "          [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
       "          [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
       "          [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\n",
       "          [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
       "          [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]],\n",
       "  \n",
       "         [[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "          [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "          [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
       "          [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
       "          [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
       "          [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
       "          [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
       "          [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\n",
       "          [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
       "          [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]],\n",
       "  \n",
       "         [[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "          [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "          [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
       "          [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
       "          [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
       "          [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
       "          [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
       "          [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\n",
       "          [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
       "          [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]],\n",
       "  \n",
       "         [[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "          [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "          [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
       "          [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
       "          [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
       "          [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
       "          [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
       "          [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\n",
       "          [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
       "          [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]]]),\n",
       "  'bias': array([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
       "         [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
       "         [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
       "         [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])},\n",
       " {'weight': array([[[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "          [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "          [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
       "          [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
       "          [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
       "          [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
       "          [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
       "          [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\n",
       "          [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
       "          [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]],\n",
       "  \n",
       "         [[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "          [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "          [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
       "          [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
       "          [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
       "          [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
       "          [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
       "          [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\n",
       "          [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
       "          [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]],\n",
       "  \n",
       "         [[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "          [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "          [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
       "          [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
       "          [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
       "          [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
       "          [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
       "          [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\n",
       "          [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
       "          [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]],\n",
       "  \n",
       "         [[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "          [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "          [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
       "          [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
       "          [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
       "          [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
       "          [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
       "          [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\n",
       "          [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
       "          [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]]]),\n",
       "  'bias': array([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
       "         [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
       "         [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
       "         [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_head = 4\n",
    "d_attn = 10\n",
    "d_in = 10\n",
    "d_out = 10\n",
    "query_weight_dict, key_weight_dict, value_weight_dict = multi_head_weights(n_head, d_attn, d_in, d_out)\n",
    "query_weight_dict, key_weight_dict, value_weight_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define a function thate generates combine weights and bias.  \n",
    "Its `d_in` is  `n_head` * `d_out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_weights, combine_bias = generate_weight_bias(n_head*d_out, d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 40)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "\n",
    "### 1. Calcuate attention score for each head\n",
    "\n",
    "Define a function that implements below:\n",
    "\n",
    "For $h \\in [H]$  \n",
    "    $\\bf{Y}^{h} \\gets \\text{Attention}(\\bf{X},\\bf{Z}|\\bf{W}_{qkv}^{h},\\text{Mask})$\n",
    "\n",
    "*Hint*: use `masked_attention` for each head and concat results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_masked_attention(\n",
    "    current_matrix,\n",
    "    context_matrix,\n",
    "    mask_matrix,\n",
    "    query_weight_dict,\n",
    "    key_weight_dict,\n",
    "    value_weight_dict,\n",
    "    d_attn,\n",
    "    n_head,\n",
    "):\n",
    "    query_weights = query_weight_dict[\"weight\"]\n",
    "    key_weights = key_weight_dict[\"weight\"]\n",
    "    value_weights = value_weight_dict[\"weight\"]\n",
    "    query_biases = query_weight_dict[\"bias\"]\n",
    "    key_biases = key_weight_dict[\"bias\"]\n",
    "    value_biases = value_weight_dict[\"bias\"]\n",
    "    multi_head_outputs = []\n",
    "    for i in range(n_head):\n",
    "        masked_output = masked_attention(\n",
    "            current_matrix,\n",
    "            context_matrix,\n",
    "            mask_matrix,\n",
    "            query_weights[i],\n",
    "            key_weights[i],\n",
    "            value_weights[i],\n",
    "            query_biases[i],\n",
    "            key_biases[i],\n",
    "            value_biases[i],\n",
    "            d_attn,\n",
    "        )\n",
    "        multi_head_outputs.append(masked_output)\n",
    "    return np.stack(multi_head_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 33.15053763,  66.30107527,  99.4516129 , 132.60215054,\n",
       "         165.75268817, 198.90322581, 232.05376344, 265.20430108,\n",
       "         298.35483871, 331.50537634],\n",
       "        [ 42.62601626,  85.25203252, 127.87804878, 170.50406504,\n",
       "         213.1300813 , 255.75609756, 298.38211382, 341.00813008,\n",
       "         383.63414634, 426.2601626 ],\n",
       "        [ 52.30718954, 104.61437908, 156.92156863, 209.22875817,\n",
       "         261.53594771, 313.84313725, 366.1503268 , 418.45751634,\n",
       "         470.76470588, 523.07189542]],\n",
       "\n",
       "       [[ 33.15053763,  66.30107527,  99.4516129 , 132.60215054,\n",
       "         165.75268817, 198.90322581, 232.05376344, 265.20430108,\n",
       "         298.35483871, 331.50537634],\n",
       "        [ 42.62601626,  85.25203252, 127.87804878, 170.50406504,\n",
       "         213.1300813 , 255.75609756, 298.38211382, 341.00813008,\n",
       "         383.63414634, 426.2601626 ],\n",
       "        [ 52.30718954, 104.61437908, 156.92156863, 209.22875817,\n",
       "         261.53594771, 313.84313725, 366.1503268 , 418.45751634,\n",
       "         470.76470588, 523.07189542]],\n",
       "\n",
       "       [[ 33.15053763,  66.30107527,  99.4516129 , 132.60215054,\n",
       "         165.75268817, 198.90322581, 232.05376344, 265.20430108,\n",
       "         298.35483871, 331.50537634],\n",
       "        [ 42.62601626,  85.25203252, 127.87804878, 170.50406504,\n",
       "         213.1300813 , 255.75609756, 298.38211382, 341.00813008,\n",
       "         383.63414634, 426.2601626 ],\n",
       "        [ 52.30718954, 104.61437908, 156.92156863, 209.22875817,\n",
       "         261.53594771, 313.84313725, 366.1503268 , 418.45751634,\n",
       "         470.76470588, 523.07189542]],\n",
       "\n",
       "       [[ 33.15053763,  66.30107527,  99.4516129 , 132.60215054,\n",
       "         165.75268817, 198.90322581, 232.05376344, 265.20430108,\n",
       "         298.35483871, 331.50537634],\n",
       "        [ 42.62601626,  85.25203252, 127.87804878, 170.50406504,\n",
       "         213.1300813 , 255.75609756, 298.38211382, 341.00813008,\n",
       "         383.63414634, 426.2601626 ],\n",
       "        [ 52.30718954, 104.61437908, 156.92156863, 209.22875817,\n",
       "         261.53594771, 313.84313725, 366.1503268 , 418.45751634,\n",
       "         470.76470588, 523.07189542]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_outputs = multi_head_masked_attention(\n",
    "    current_matrix,\n",
    "    context_matrix,\n",
    "    mask_matrix,\n",
    "    query_weight_dict,\n",
    "    key_weight_dict,\n",
    "    value_weight_dict,\n",
    "    d_attn,\n",
    "    n_head,\n",
    ")\n",
    "multi_head_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of `multi_head_outputs` is equal to `n_head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(multi_head_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Concat the result of each head\n",
    "\n",
    "Define a function that implements below:\n",
    "\n",
    "$$\n",
    "\\bf{Y} \\gets [\\bf{Y}^{1}; \\bf{Y}^{2}; ...;\\bf{Y}^{H}]\n",
    "$$\n",
    "\n",
    "Concat the result of each head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_multi_head(multi_head_outputs):\n",
    "    multi_head_outputs_t = multi_head_outputs.transpose(1,0,2)\n",
    "    size = len(multi_head_outputs_t)\n",
    "    multi_head_outputs_t = multi_head_outputs_t.reshape(size, -1)\n",
    "    return multi_head_outputs_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 33.15053763,  66.30107527,  99.4516129 , 132.60215054,\n",
       "        165.75268817, 198.90322581, 232.05376344, 265.20430108,\n",
       "        298.35483871, 331.50537634,  33.15053763,  66.30107527,\n",
       "         99.4516129 , 132.60215054, 165.75268817, 198.90322581,\n",
       "        232.05376344, 265.20430108, 298.35483871, 331.50537634,\n",
       "         33.15053763,  66.30107527,  99.4516129 , 132.60215054,\n",
       "        165.75268817, 198.90322581, 232.05376344, 265.20430108,\n",
       "        298.35483871, 331.50537634,  33.15053763,  66.30107527,\n",
       "         99.4516129 , 132.60215054, 165.75268817, 198.90322581,\n",
       "        232.05376344, 265.20430108, 298.35483871, 331.50537634],\n",
       "       [ 42.62601626,  85.25203252, 127.87804878, 170.50406504,\n",
       "        213.1300813 , 255.75609756, 298.38211382, 341.00813008,\n",
       "        383.63414634, 426.2601626 ,  42.62601626,  85.25203252,\n",
       "        127.87804878, 170.50406504, 213.1300813 , 255.75609756,\n",
       "        298.38211382, 341.00813008, 383.63414634, 426.2601626 ,\n",
       "         42.62601626,  85.25203252, 127.87804878, 170.50406504,\n",
       "        213.1300813 , 255.75609756, 298.38211382, 341.00813008,\n",
       "        383.63414634, 426.2601626 ,  42.62601626,  85.25203252,\n",
       "        127.87804878, 170.50406504, 213.1300813 , 255.75609756,\n",
       "        298.38211382, 341.00813008, 383.63414634, 426.2601626 ],\n",
       "       [ 52.30718954, 104.61437908, 156.92156863, 209.22875817,\n",
       "        261.53594771, 313.84313725, 366.1503268 , 418.45751634,\n",
       "        470.76470588, 523.07189542,  52.30718954, 104.61437908,\n",
       "        156.92156863, 209.22875817, 261.53594771, 313.84313725,\n",
       "        366.1503268 , 418.45751634, 470.76470588, 523.07189542,\n",
       "         52.30718954, 104.61437908, 156.92156863, 209.22875817,\n",
       "        261.53594771, 313.84313725, 366.1503268 , 418.45751634,\n",
       "        470.76470588, 523.07189542,  52.30718954, 104.61437908,\n",
       "        156.92156863, 209.22875817, 261.53594771, 313.84313725,\n",
       "        366.1503268 , 418.45751634, 470.76470588, 523.07189542]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_multi_head_outputs = concat_multi_head(multi_head_outputs)\n",
    "concat_multi_head_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`concat_multi_head_outputs` shape must be looks like `len(current_matrix)`, `n_head`*`n_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 40)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_multi_head_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 40)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(current_matrix), n_head*d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Combine concated result with weight and bias\n",
    "\n",
    "Define a function that implements below:\n",
    "\n",
    "$$\n",
    "\\bf{Y} \\gets [\\bf{Y}^{1}; \\bf{Y}^{2}; ...;\\bf{Y}^{H}]\n",
    "$$\n",
    "\n",
    "Concat the result of each head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_multi_head_outputs(concat_multi_head_outputs, combine_weights, combine_bias):\n",
    "    combined_outputs = combine_weights.dot(concat_multi_head_outputs.T).T + combine_bias\n",
    "    return combined_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7294.11827957,  14588.23655914,  21882.35483871,\n",
       "         29176.47311828,  36470.59139785,  43764.70967742,\n",
       "         51058.82795699,  58352.94623656,  65647.06451613,\n",
       "         72941.1827957 ],\n",
       "       [  9378.72357724,  18757.44715447,  28136.17073171,\n",
       "         37514.89430894,  46893.61788618,  56272.34146341,\n",
       "         65651.06504065,  75029.78861789,  84408.51219512,\n",
       "         93787.23577236],\n",
       "       [ 11508.58169935,  23017.16339869,  34525.74509804,\n",
       "         46034.32679739,  57542.90849673,  69051.49019608,\n",
       "         80560.07189542,  92068.65359477, 103577.23529412,\n",
       "        115085.81699346]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_outputs = combine_multi_head_outputs(concat_multi_head_outputs, combine_weights, combine_bias)\n",
    "combined_outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2856a52edefdf1612ecc51b9a351ad923998c55f10cb9b041920567e71365ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
